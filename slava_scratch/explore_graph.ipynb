{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f2508ca5ed0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tutils\n",
    "from transformer_lens.evals import make_pile_data_loader, evaluate_on_dataset\n",
    "\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import einops\n",
    "\n",
    "from sae_lens import SAE\n",
    "# from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "# from sae_lens import SparseAutoencoder, ActivationsStore\n",
    "\n",
    "from steering.evals_utils import evaluate_completions, multi_criterion_evaluation\n",
    "from steering.utils import normalise_decoder, text_to_sae_feats, top_activations\n",
    "from steering.patch import generate, scores_2d, patch_resid\n",
    "\n",
    "# from sae_vis.data_config_classes import SaeVisConfig\n",
    "# from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e4e8583fb3433eac5599dfc5ca3764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp6 = \"blocks.6.hook_resid_post\"\n",
    "\n",
    "sae6, _, _ = SAE.from_pretrained(\n",
    "    release = \"gemma-2b-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = hp6, # won't always be a hook point\n",
    "    device = 'cpu'\n",
    ")\n",
    "\n",
    "sae6 = sae6.to(device)\n",
    "normalise_decoder(sae6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_is = torch.load(\"top_is.pt\")\n",
    "forward_vs = torch.load(\"top_vs.pt\")\n",
    "fixed_is = []\n",
    "for row in forward_is:\n",
    "    if (row == 0).all():\n",
    "        fixed_is.append(-torch.ones_like(row))\n",
    "    else:\n",
    "        fixed_is.append(row)\n",
    "forward_is = torch.stack(fixed_is)\n",
    "del fixed_is\n",
    "\n",
    "backward_is = torch.load(\"reverse_is.pt\")\n",
    "backward_vs = torch.load(\"reverse_vs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10138, tensor(1.1502)),\n",
       " (4593, tensor(1.0046)),\n",
       " (10655, tensor(0.9132)),\n",
       " (10148, tensor(0.9097)),\n",
       " (1852, tensor(0.8904)),\n",
       " (15582, tensor(0.8652)),\n",
       " (13568, tensor(0.8604)),\n",
       " (8305, tensor(0.8113)),\n",
       " (1295, tensor(0.8106)),\n",
       " (8370, tensor(0.8097)),\n",
       " (10286, tensor(0.7876)),\n",
       " (2554, tensor(0.7197)),\n",
       " (5932, tensor(0.6843)),\n",
       " (16247, tensor(0.6818)),\n",
       " (3245, tensor(0.6674)),\n",
       " (1822, tensor(0.6376)),\n",
       " (4479, tensor(0.6095)),\n",
       " (13981, tensor(0.5894)),\n",
       " (9043, tensor(0.5766)),\n",
       " (10567, tensor(0.5431)),\n",
       " (10702, tensor(0.5418)),\n",
       " (6318, tensor(0.5360)),\n",
       " (1178, tensor(0.5339)),\n",
       " (5823, tensor(0.5169)),\n",
       " (12382, tensor(0.4915)),\n",
       " (7339, tensor(0.4712)),\n",
       " (8005, tensor(0.4693)),\n",
       " (11061, tensor(0.4691)),\n",
       " (11786, tensor(0.4606)),\n",
       " (13694, tensor(0.4365)),\n",
       " (15451, tensor(0.4357)),\n",
       " (9235, tensor(0.4338)),\n",
       " (4152, tensor(0.4184)),\n",
       " (15548, tensor(0.4096)),\n",
       " (543, tensor(0.4065)),\n",
       " (8516, tensor(0.4057)),\n",
       " (68, tensor(0.3763)),\n",
       " (536, tensor(0.3748)),\n",
       " (13291, tensor(0.3662)),\n",
       " (4616, tensor(0.3312)),\n",
       " (10243, tensor(0.3275)),\n",
       " (12090, tensor(0.3144)),\n",
       " (11518, tensor(0.2957)),\n",
       " (15802, tensor(0.2936)),\n",
       " (5037, tensor(0.2841)),\n",
       " (15044, tensor(0.2704)),\n",
       " (13095, tensor(0.2546)),\n",
       " (4235, tensor(0.2545)),\n",
       " (3235, tensor(0.2288)),\n",
       " (13485, tensor(0.2245)),\n",
       " (13873, tensor(0.2107)),\n",
       " (4515, tensor(0.2052)),\n",
       " (6248, tensor(0.1947)),\n",
       " (4655, tensor(0.1732)),\n",
       " (1311, tensor(0.1703)),\n",
       " (10140, tensor(0.1460)),\n",
       " (8281, tensor(0.1420)),\n",
       " (9861, tensor(0.1415)),\n",
       " (583, tensor(0.1396)),\n",
       " (2157, tensor(0.1391)),\n",
       " (15652, tensor(0.1386)),\n",
       " (7012, tensor(0.1381)),\n",
       " (8224, tensor(0.1342)),\n",
       " (4723, tensor(0.1325)),\n",
       " (6584, tensor(0.1308)),\n",
       " (1411, tensor(0.1302)),\n",
       " (7329, tensor(0.1277)),\n",
       " (7142, tensor(0.1269)),\n",
       " (11420, tensor(0.1261)),\n",
       " (15628, tensor(0.1203)),\n",
       " (10352, tensor(0.1202)),\n",
       " (11975, tensor(0.1182)),\n",
       " (14348, tensor(0.1171)),\n",
       " (3043, tensor(0.1166)),\n",
       " (5451, tensor(0.1165)),\n",
       " (5091, tensor(0.1164)),\n",
       " (9914, tensor(0.1162)),\n",
       " (2569, tensor(0.1157)),\n",
       " (13523, tensor(0.1152)),\n",
       " (7068, tensor(0.1129)),\n",
       " (3015, tensor(0.1119)),\n",
       " (2403, tensor(0.1101)),\n",
       " (13679, tensor(0.1098)),\n",
       " (7356, tensor(0.1097)),\n",
       " (8431, tensor(0.1094)),\n",
       " (11444, tensor(0.1094)),\n",
       " (8760, tensor(0.1093)),\n",
       " (15407, tensor(0.1059)),\n",
       " (15465, tensor(0.1051)),\n",
       " (1306, tensor(0.1034)),\n",
       " (6003, tensor(0.1009)),\n",
       " (424, tensor(0.0955)),\n",
       " (10326, tensor(0.0954)),\n",
       " (1261, tensor(0.0949)),\n",
       " (11355, tensor(0.0945)),\n",
       " (244, tensor(0.0922)),\n",
       " (1598, tensor(0.0918)),\n",
       " (7154, tensor(0.0916)),\n",
       " (10186, tensor(0.0913)),\n",
       " (378, tensor(0.0911)),\n",
       " (14474, tensor(0.0909)),\n",
       " (16322, tensor(0.0887)),\n",
       " (6802, tensor(0.0883)),\n",
       " (7604, tensor(0.0837)),\n",
       " (5845, tensor(0.0837)),\n",
       " (11404, tensor(0.0835)),\n",
       " (10405, tensor(0.0827)),\n",
       " (6596, tensor(0.0819)),\n",
       " (5307, tensor(0.0813)),\n",
       " (9888, tensor(0.0811)),\n",
       " (8200, tensor(0.0806)),\n",
       " (10210, tensor(0.0798)),\n",
       " (4564, tensor(0.0793)),\n",
       " (7850, tensor(0.0789)),\n",
       " (15494, tensor(0.0786)),\n",
       " (145, tensor(0.0786)),\n",
       " (15995, tensor(0.0780)),\n",
       " (1865, tensor(0.0772)),\n",
       " (825, tensor(0.0772)),\n",
       " (11018, tensor(0.0764)),\n",
       " (4178, tensor(0.0752)),\n",
       " (2368, tensor(0.0748)),\n",
       " (10267, tensor(0.0744)),\n",
       " (15453, tensor(0.0743)),\n",
       " (3523, tensor(0.0742)),\n",
       " (11820, tensor(0.0740)),\n",
       " (11467, tensor(0.0738)),\n",
       " (1840, tensor(0.0717)),\n",
       " (7492, tensor(0.0714)),\n",
       " (8969, tensor(0.0710)),\n",
       " (9762, tensor(0.0698)),\n",
       " (735, tensor(0.0695)),\n",
       " (13247, tensor(0.0695)),\n",
       " (9505, tensor(0.0694)),\n",
       " (15729, tensor(0.0694)),\n",
       " (8711, tensor(0.0693)),\n",
       " (1925, tensor(0.0692)),\n",
       " (5155, tensor(0.0682)),\n",
       " (8320, tensor(0.0678)),\n",
       " (2948, tensor(0.0677)),\n",
       " (6490, tensor(0.0673)),\n",
       " (6958, tensor(0.0672)),\n",
       " (13507, tensor(0.0669)),\n",
       " (11233, tensor(0.0667)),\n",
       " (9667, tensor(0.0666)),\n",
       " (15550, tensor(0.0662)),\n",
       " (12640, tensor(0.0657)),\n",
       " (1930, tensor(0.0654)),\n",
       " (9201, tensor(0.0654)),\n",
       " (7376, tensor(0.0652)),\n",
       " (8504, tensor(0.0639)),\n",
       " (14377, tensor(0.0634)),\n",
       " (8006, tensor(0.0630)),\n",
       " (3206, tensor(0.0619)),\n",
       " (7099, tensor(0.0613)),\n",
       " (8020, tensor(0.0599)),\n",
       " (11734, tensor(0.0597)),\n",
       " (12696, tensor(0.0597)),\n",
       " (12612, tensor(0.0595)),\n",
       " (10146, tensor(0.0588)),\n",
       " (14582, tensor(0.0583)),\n",
       " (8444, tensor(0.0565)),\n",
       " (1923, tensor(0.0560)),\n",
       " (7967, tensor(0.0560)),\n",
       " (396, tensor(0.0548)),\n",
       " (15505, tensor(0.0546)),\n",
       " (7009, tensor(0.0544)),\n",
       " (6820, tensor(0.0537)),\n",
       " (13667, tensor(0.0537)),\n",
       " (8922, tensor(0.0530)),\n",
       " (3022, tensor(0.0521)),\n",
       " (8464, tensor(0.0517)),\n",
       " (9036, tensor(0.0514)),\n",
       " (7889, tensor(0.0514)),\n",
       " (12705, tensor(0.0512)),\n",
       " (6685, tensor(0.0510)),\n",
       " (12024, tensor(0.0510)),\n",
       " (2797, tensor(0.0509)),\n",
       " (5594, tensor(0.0508)),\n",
       " (13035, tensor(0.0504)),\n",
       " (1908, tensor(0.0503)),\n",
       " (8636, tensor(0.0503)),\n",
       " (9460, tensor(0.0500)),\n",
       " (10905, tensor(0.0495)),\n",
       " (9473, tensor(0.0494)),\n",
       " (4893, tensor(0.0492)),\n",
       " (7705, tensor(0.0491)),\n",
       " (2933, tensor(0.0486)),\n",
       " (5608, tensor(0.0483)),\n",
       " (7562, tensor(0.0474)),\n",
       " (11986, tensor(0.0473)),\n",
       " (12951, tensor(0.0460)),\n",
       " (12324, tensor(0.0451)),\n",
       " (12783, tensor(0.0451)),\n",
       " (11578, tensor(0.0451)),\n",
       " (347, tensor(0.0447)),\n",
       " (10270, tensor(0.0447)),\n",
       " (13983, tensor(0.0441)),\n",
       " (10727, tensor(0.0422)),\n",
       " (15574, tensor(0.0412)),\n",
       " (12749, tensor(0.0407)),\n",
       " (5983, tensor(0.0405)),\n",
       " (8512, tensor(0.0396)),\n",
       " (15392, tensor(0.0391)),\n",
       " (8621, tensor(0.0390)),\n",
       " (15884, tensor(0.0384)),\n",
       " (9871, tensor(0.0382)),\n",
       " (11135, tensor(0.0380)),\n",
       " (14160, tensor(0.0373)),\n",
       " (2450, tensor(0.0372)),\n",
       " (13104, tensor(0.0369)),\n",
       " (10346, tensor(0.0365)),\n",
       " (1179, tensor(0.0365)),\n",
       " (5247, tensor(0.0355)),\n",
       " (14492, tensor(0.0348)),\n",
       " (488, tensor(0.0347)),\n",
       " (4323, tensor(0.0338)),\n",
       " (1451, tensor(0.0322)),\n",
       " (3737, tensor(0.0309)),\n",
       " (14667, tensor(0.0308)),\n",
       " (907, tensor(0.0298)),\n",
       " (11334, tensor(0.0295)),\n",
       " (5537, tensor(0.0292)),\n",
       " (5609, tensor(0.0282)),\n",
       " (15794, tensor(0.0281)),\n",
       " (4924, tensor(0.0276)),\n",
       " (13828, tensor(0.0275)),\n",
       " (15957, tensor(0.0261)),\n",
       " (10778, tensor(0.0260)),\n",
       " (3375, tensor(0.0259)),\n",
       " (7284, tensor(0.0223)),\n",
       " (12132, tensor(0.0198)),\n",
       " (4703, tensor(0.0186)),\n",
       " (12048, tensor(0.0180)),\n",
       " (12283, tensor(0.0179)),\n",
       " (8049, tensor(0.0179)),\n",
       " (6651, tensor(0.0173)),\n",
       " (10799, tensor(0.0167)),\n",
       " (3427, tensor(0.0161)),\n",
       " (15331, tensor(0.0159)),\n",
       " (11310, tensor(0.0158)),\n",
       " (9104, tensor(0.0157)),\n",
       " (820, tensor(0.0154)),\n",
       " (15247, tensor(0.0147)),\n",
       " (11574, tensor(0.0141)),\n",
       " (671, tensor(0.0140)),\n",
       " (3241, tensor(0.0126)),\n",
       " (15546, tensor(0.0122)),\n",
       " (6255, tensor(0.0113)),\n",
       " (388, tensor(0.0112)),\n",
       " (9695, tensor(0.0101)),\n",
       " (6427, tensor(0.0092)),\n",
       " (11418, tensor(0.0092)),\n",
       " (3570, tensor(0.0089)),\n",
       " (11229, tensor(0.0082)),\n",
       " (1188, tensor(0.0081)),\n",
       " (10451, tensor(0.0070)),\n",
       " (1989, tensor(0.0061))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_act_siblings(act_id):\n",
    "    acts_on = forward_is[act_id]\n",
    "    siblings = defaultdict(float)\n",
    "    for i, act_on in enumerate(acts_on):\n",
    "        if act_on == -1:\n",
    "            continue\n",
    "\n",
    "        in_val = forward_vs[act_id][i]\n",
    "        for j, sibling in enumerate(backward_is[act_on]):\n",
    "            if sibling == -1:\n",
    "                continue\n",
    "            siblings[sibling.item()] += in_val * backward_vs[act_on][j].item()\n",
    "        \n",
    "    siblings = [(k, v) for k, v in siblings.items()]\n",
    "    siblings.sort(key=lambda x: x[1], reverse=True)\n",
    "    return siblings\n",
    "\n",
    "\n",
    "find_act_siblings(10138) # london\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10138, tensor(2.1266)),\n",
       " (10148, tensor(2.0577)),\n",
       " (8370, tensor(1.8711)),\n",
       " (15505, tensor(1.4334)),\n",
       " (4152, tensor(1.0667)),\n",
       " (4343, tensor(0.8933)),\n",
       " (12775, tensor(0.8817)),\n",
       " (15231, tensor(0.8519)),\n",
       " (14640, tensor(0.8457)),\n",
       " (10655, tensor(0.8100)),\n",
       " (12090, tensor(0.8000)),\n",
       " (3174, tensor(0.6445)),\n",
       " (13841, tensor(0.4994)),\n",
       " (14112, tensor(0.4897)),\n",
       " (4235, tensor(0.4668)),\n",
       " (4323, tensor(0.4582)),\n",
       " (11057, tensor(0.4471)),\n",
       " (11832, tensor(0.4282)),\n",
       " (8706, tensor(0.4260)),\n",
       " (13282, tensor(0.4253)),\n",
       " (11444, tensor(0.4093)),\n",
       " (5096, tensor(0.3891)),\n",
       " (10207, tensor(0.3793)),\n",
       " (9104, tensor(0.3756)),\n",
       " (4542, tensor(0.3510)),\n",
       " (918, tensor(0.3440)),\n",
       " (5168, tensor(0.3272)),\n",
       " (2507, tensor(0.3252)),\n",
       " (4989, tensor(0.3006)),\n",
       " (3985, tensor(0.2933)),\n",
       " (7068, tensor(0.2840)),\n",
       " (7329, tensor(0.2796)),\n",
       " (1889, tensor(0.2507)),\n",
       " (13694, tensor(0.2466)),\n",
       " (2288, tensor(0.2307)),\n",
       " (9320, tensor(0.2277)),\n",
       " (11014, tensor(0.2217)),\n",
       " (10905, tensor(0.2217)),\n",
       " (11376, tensor(0.2113)),\n",
       " (13568, tensor(0.2093)),\n",
       " (8882, tensor(0.2077)),\n",
       " (2107, tensor(0.1953)),\n",
       " (11912, tensor(0.1951)),\n",
       " (10871, tensor(0.1934)),\n",
       " (7801, tensor(0.1878)),\n",
       " (422, tensor(0.1877)),\n",
       " (2770, tensor(0.1828)),\n",
       " (9888, tensor(0.1828)),\n",
       " (6090, tensor(0.1817)),\n",
       " (14914, tensor(0.1789)),\n",
       " (3013, tensor(0.1766)),\n",
       " (4284, tensor(0.1761)),\n",
       " (2725, tensor(0.1745)),\n",
       " (15173, tensor(0.1681)),\n",
       " (6331, tensor(0.1572)),\n",
       " (1584, tensor(0.1572)),\n",
       " (9190, tensor(0.1557)),\n",
       " (9526, tensor(0.1557)),\n",
       " (8922, tensor(0.1514)),\n",
       " (5523, tensor(0.1508)),\n",
       " (7455, tensor(0.1475)),\n",
       " (5722, tensor(0.1465)),\n",
       " (5609, tensor(0.1351)),\n",
       " (4084, tensor(0.1331)),\n",
       " (13485, tensor(0.1247)),\n",
       " (12382, tensor(0.1210)),\n",
       " (6554, tensor(0.1188)),\n",
       " (15102, tensor(0.1155)),\n",
       " (4828, tensor(0.0999)),\n",
       " (11255, tensor(0.0997)),\n",
       " (7775, tensor(0.0993)),\n",
       " (6248, tensor(0.0971)),\n",
       " (1207, tensor(0.0968)),\n",
       " (5823, tensor(0.0950)),\n",
       " (9670, tensor(0.0927)),\n",
       " (11023, tensor(0.0917)),\n",
       " (12803, tensor(0.0875)),\n",
       " (10326, tensor(0.0871)),\n",
       " (15135, tensor(0.0870)),\n",
       " (3574, tensor(0.0864)),\n",
       " (8693, tensor(0.0862)),\n",
       " (8426, tensor(0.0813)),\n",
       " (7656, tensor(0.0808)),\n",
       " (7768, tensor(0.0782)),\n",
       " (8200, tensor(0.0764)),\n",
       " (8381, tensor(0.0757)),\n",
       " (9905, tensor(0.0757)),\n",
       " (6802, tensor(0.0749)),\n",
       " (2157, tensor(0.0748)),\n",
       " (3927, tensor(0.0713)),\n",
       " (1019, tensor(0.0698)),\n",
       " (729, tensor(0.0675)),\n",
       " (11847, tensor(0.0652)),\n",
       " (599, tensor(0.0649)),\n",
       " (13888, tensor(0.0645)),\n",
       " (10936, tensor(0.0634)),\n",
       " (10501, tensor(0.0630)),\n",
       " (6129, tensor(0.0614)),\n",
       " (4079, tensor(0.0599)),\n",
       " (13318, tensor(0.0556)),\n",
       " (13314, tensor(0.0528)),\n",
       " (7350, tensor(0.0527)),\n",
       " (13069, tensor(0.0517)),\n",
       " (14001, tensor(0.0504)),\n",
       " (10171, tensor(0.0467)),\n",
       " (4114, tensor(0.0447)),\n",
       " (6077, tensor(0.0445)),\n",
       " (7154, tensor(0.0429)),\n",
       " (1411, tensor(0.0419)),\n",
       " (15601, tensor(0.0396)),\n",
       " (10722, tensor(0.0384)),\n",
       " (1642, tensor(0.0276)),\n",
       " (4658, tensor(0.0274)),\n",
       " (2813, tensor(0.0264)),\n",
       " (6922, tensor(0.0232)),\n",
       " (613, tensor(0.0225)),\n",
       " (15560, tensor(0.0213)),\n",
       " (1200, tensor(0.0205)),\n",
       " (14786, tensor(0.0195)),\n",
       " (11668, tensor(0.0183)),\n",
       " (4354, tensor(0.0181)),\n",
       " (9054, tensor(0.0181)),\n",
       " (13983, tensor(0.0178)),\n",
       " (13667, tensor(0.0168)),\n",
       " (3529, tensor(0.0160)),\n",
       " (713, tensor(0.0156)),\n",
       " (2713, tensor(0.0155)),\n",
       " (7591, tensor(0.0154)),\n",
       " (16118, tensor(0.0147)),\n",
       " (3206, tensor(0.0146)),\n",
       " (9635, tensor(0.0145)),\n",
       " (2500, tensor(0.0139)),\n",
       " (8399, tensor(0.0126)),\n",
       " (8227, tensor(0.0125)),\n",
       " (11681, tensor(0.0124)),\n",
       " (9387, tensor(0.0110)),\n",
       " (11903, tensor(0.0103)),\n",
       " (13181, tensor(0.0102)),\n",
       " (14147, tensor(0.0101)),\n",
       " (2151, tensor(0.0100)),\n",
       " (11522, tensor(0.0099)),\n",
       " (2595, tensor(0.0098)),\n",
       " (1414, tensor(0.0095)),\n",
       " (4245, tensor(0.0094)),\n",
       " (14182, tensor(0.0093)),\n",
       " (5546, tensor(0.0090)),\n",
       " (14038, tensor(0.0090)),\n",
       " (13523, tensor(0.0085)),\n",
       " (11541, tensor(0.0082)),\n",
       " (12116, tensor(0.0080)),\n",
       " (7881, tensor(0.0079)),\n",
       " (15142, tensor(0.0074)),\n",
       " (5225, tensor(0.0061)),\n",
       " (8214, tensor(0.0061)),\n",
       " (9548, tensor(0.0060)),\n",
       " (163, tensor(0.0060)),\n",
       " (1825, tensor(0.0056)),\n",
       " (1750, tensor(0.0055)),\n",
       " (5123, tensor(0.0054)),\n",
       " (13076, tensor(0.0052)),\n",
       " (16097, tensor(0.0051)),\n",
       " (4178, tensor(0.0047)),\n",
       " (391, tensor(0.0044)),\n",
       " (2060, tensor(0.0042)),\n",
       " (12796, tensor(0.0037)),\n",
       " (2368, tensor(0.0037)),\n",
       " (12808, tensor(0.0037)),\n",
       " (1068, tensor(0.0036)),\n",
       " (11803, tensor(0.0036)),\n",
       " (2482, tensor(0.0035)),\n",
       " (2745, tensor(0.0035)),\n",
       " (7631, tensor(0.0035))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_effect_siblings(effect_id):\n",
    "    parents = backward_is[effect_id]\n",
    "    siblings = defaultdict(float)\n",
    "\n",
    "    for i, parent in enumerate(parents):\n",
    "        if parent == -1:\n",
    "            continue\n",
    "\n",
    "        in_val = backward_vs[effect_id][i]\n",
    "        for j, sibling in enumerate(forward_is[parent]):\n",
    "            if sibling == -1:\n",
    "                continue\n",
    "            siblings[sibling.item()] += in_val * forward_vs[parent][j].item()\n",
    "\n",
    "    siblings = [(k, v) for k, v in siblings.items()]\n",
    "    siblings.sort(key=lambda x: x[1], reverse=True)\n",
    "    return siblings\n",
    "\n",
    "find_effect_siblings(10138) # london"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
