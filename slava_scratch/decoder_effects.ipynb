{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at what happens to the encoder when we inject a decoder vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f242f62c820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tutils\n",
    "from transformer_lens.evals import make_pile_data_loader, evaluate_on_dataset\n",
    "\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import einops\n",
    "\n",
    "from sae_lens import SAE\n",
    "# from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "# from sae_lens import SparseAutoencoder, ActivationsStore\n",
    "\n",
    "from steering.evals_utils import evaluate_completions, multi_criterion_evaluation\n",
    "from steering.utils import normalise_decoder\n",
    "from steering.patch import generate, scores_2d, patch_resid\n",
    "\n",
    "# from sae_vis.data_config_classes import SaeVisConfig\n",
    "# from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558dd16ae70741018d905956ae546b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp6 = \"blocks.6.hook_resid_post\"\n",
    "\n",
    "sae6, _, _ = SAE.from_pretrained(\n",
    "    release = \"gemma-2b-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = hp6, # won't always be a hook point\n",
    "    device = 'cpu'\n",
    ")\n",
    "\n",
    "sae6 = sae6.to(device)\n",
    "normalise_decoder(sae6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intelligence = sae6.W_dec[10351]   # intelligence and genius\n",
    "writing = sae6.W_dec[1058]  # writing\n",
    "anger = sae6.W_dec[1062]  # anger\n",
    "london = sae6.W_dec[10138]  # London\n",
    "wedding = sae6.W_dec[8406]  # wedding\n",
    "broad_wedding = sae6.W_dec[2378] # broad wedding\n",
    "\n",
    "broad_poetry = sae6.W_dec[11067]  # broad poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer = writing\n",
    "scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "tokenized_data = tutils.tokenize_and_concatenate(data, model.tokenizer, max_length=32)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "loader = DataLoader(tokenized_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10\n",
    "\n",
    "diffs = torch.zeros(sae6.cfg.d_sae, device=sae6.W_dec.device)\n",
    "\n",
    "for batch_idx, batch in enumerate(loader):\n",
    "\n",
    "    _, acts = model.run_with_cache(batch['tokens'], names_filter=hp6)\n",
    "    acts = acts[hp6]\n",
    "    acts = acts.reshape(-1, acts.shape[-1]) \n",
    "\n",
    "    pre_distribution = sae6.encode(acts)\n",
    "    post_distribution = sae6.encode(acts + steer*scale)\n",
    "\n",
    "    diff = post_distribution - pre_distribution\n",
    "    diffs += diff.sum(dim=0)\n",
    "\n",
    "    if batch_idx >= n_steps - 1:\n",
    "        break\n",
    "\n",
    "diffs /= (n_steps*32*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1058, 13838,  9203, 11338,  7050,  1988,   650, 10345,  2151,  1852],\n",
      "       device='cuda:0')\n",
      "tensor([94.1977,  5.8552,  4.6552,  2.8302,  2.3867,  2.0325,  1.8691,  1.7715,\n",
      "         1.7508,  1.5424], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "top_v, top_i = torch.topk(diffs, 10, dim=-1)\n",
    "print(top_i)\n",
    "print(top_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14376, 16277, 10971,  8322, 12520,  3659,  6037, 16150,  2229,  9278],\n",
      "       device='cuda:0')\n",
      "tensor([1.0137, 0.6913, 0.4905, 0.3307, 0.3178, 0.3046, 0.3045, 0.2791, 0.2618,\n",
      "        0.2464], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "bottom_v, bottom_i = torch.topk(-diffs, 10, dim=-1)\n",
    "print(bottom_i)\n",
    "print(bottom_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
