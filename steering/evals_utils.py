import json
from openai import AsyncOpenAI
from dotenv import load_dotenv
import asyncio
import nest_asyncio
import random

# Allows running asyncio in jupyter
nest_asyncio.apply()

# Load environment variables
load_dotenv()
client = AsyncOpenAI()

async def evaluate_completion(completion, criterion, prompt, client, model, verbose=False):
    system_message = "You score texts generated by a language model based on the following criterion: \n"\
        + criterion + ".\nYou provide a score from 1 to 10. \
The language model was given a prompt and generated the following text. \
Evaluate the text based on the criterion. Output format should be JSON with the following fields: \"score\" (int)"
    if verbose:
        system_message += " and \"reason\""
    
    prompt_user_text = f"Prompt:\n\n{prompt}\n\n" if prompt else ""
    
    try:
        response = await client.chat.completions.create(
            model=model,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt_user_text + f"Completion:\n\"\"\"\n{completion}\n\"\"\"\n"}
            ],
            max_tokens=150,
            temperature=0.0,
        )
        
        content = response.choices[0].message.content
        return json.loads(content)
    except json.JSONDecodeError as e:
        print(f"JSON decode error: {e}")
        print(f"Text causing the error:\n{system_message}\n{prompt}\n{completion}")
        return {"score": 0, "error": "Failed to parse JSON response"}
    except Exception as e:
        print(f"Error in evaluate_completion: {e}")
        return {"score": 0, "error": str(e)}

async def evaluate_with_timeout(completion, criterion, prompt, client, model, verbose, timeout):
    try:
        return await asyncio.wait_for(
            evaluate_completion(completion, criterion, prompt, client, model, verbose),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        return {"score": 0, "error": "Timeout"}

def evaluate_completions(
        completions: list[str],
        criterion: str,
        prompt: str,
        model="gpt-4o-mini", # gpt-4o, gpt-3.5-turbo
        verbose=False,
        ):
    async def evaluate_all():
        tasks = [
            evaluate_completion(completion, criterion, prompt, client, model, verbose=verbose)
            for completion in completions
        ]
        return await asyncio.gather(*tasks)
    
    return asyncio.run(evaluate_all())

def multi_criterion_evaluation(
        completions: list[str],
        criterions: list[str],
        prompt: str,
        model="gpt-4o-mini",
        verbose=False,
        filter_errors=True,
        print_errors=False,
        batch_size=1000,
        timeout=20,
        ):
    repeated_completions = completions * len(criterions)
    repeated_criteria = []
    for criterion in criterions:
        repeated_criteria.extend([criterion] * len(completions))
    
    async def retry_evaluate(completion, criterion, prompt, client, model, verbose, timeout, max_retries=1):
        attempts = 0
        while attempts <= max_retries:
            result = await evaluate_with_timeout(completion, criterion, prompt, client, model, verbose, timeout)
            if "error" not in result:
                return result
            elif attempts == max_retries:
                return result
            elif result.get("error") == "Timeout":
                attempts += 1
            else:
                # For other errors, don't retry
                return result

    async def evaluate_all():
        tasks = [
            retry_evaluate(completion, criterion, prompt, client, model, verbose, timeout, max_retries=1)
            for completion, criterion in zip(repeated_completions, repeated_criteria)
        ]
        # Run tasks in batches
        results = []
        for i in range(0, len(tasks), batch_size):
            chunk = tasks[i:i+batch_size]
            chunk_results = await asyncio.gather(*chunk)
            results.extend(chunk_results)
        return results
    
    results = asyncio.run(evaluate_all())

    # Reshape results into a 2D list: [completion][criterion]
    reshaped_results = []
    for i in range(len(criterions)):
        reshaped_results.append(results[i*len(completions):(i+1)*len(completions)])
    
    if not filter_errors:
        return reshaped_results
    
    # Filter out error responses and collect error statistics
    filtered_results = []
    error_counts = {}
    for criterion_results in reshaped_results:
        valid_criterion_results = []
        for result in criterion_results:
            if "error" not in result:
                valid_criterion_results.append(result)
            else:
                error_msg = result.get("error", "Unknown error")
                error_counts[error_msg] = error_counts.get(error_msg, 0) + 1
        filtered_results.append(valid_criterion_results)
    
    if print_errors and error_counts:
        error_summary = ", ".join([f"{count} '{error_msg}' errors" for error_msg, count in error_counts.items()])
        print(f"Errors occurred during evaluation: {error_summary}")
    
    return filtered_results



async def run_battle(text_1, text_2, criterion, prompt, client, model):
    """
    For Elo rating. Evaluate two texts and return the winner.
    """
    system_message = f"""You are an impartial judge tasked with comparing two text snippets generated by a language model. Your goal is to determine which text better meets the given criterion, without any bias towards either text's position or labeling.

Criterion for judgment:
{criterion}

Instructions:
1. Carefully read and analyze both texts.
2. Compare them objectively based on the given criterion.
3. Make a decision on which text better meets the criterion.
4. If there is a clear winner, choose it. If the texts are truly equal in quality, you may declare a tie.
5. Return your decision in the specified JSON format.

Your response must be in the following JSON format:
{{
  "winner": "A", "B" or "tie"
}}
"""

# Note: Set winner to "tie" only if the texts are truly equal in quality. If there's any difference, you must choose a winner."""

    prompt_user_text = f"Prompt:\n\n{prompt}\n\n" if prompt else ""
    user_message = prompt_user_text + f"""Text A:
\"\"\"
{text_1}
\"\"\"

Text B:
\"\"\"
{text_2}
\"\"\"

Based on the given criterion, which text is better? Provide your decision in the required JSON format."""

    try:
        response = await client.chat.completions.create(
            model=model,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            max_tokens=100,
            temperature=0.2,
        )

        # print()
        # print('system_message:', system_message)
        # print('user_message:', user_message)
        # print()
        
        content = response.choices[0].message.content
        result = json.loads(content)
        
        return result
    
    except json.JSONDecodeError as e:
        print(f"JSON decode error: {e}")
        print(f"Text causing the error:\n{text_1}\n{text_2}")
        return {"winner": "error", "tie": False}
    except Exception as e:
        print(f"Error in run_battle: {e}")
        return {"winner": "error", "tie": False}


def compute_battles(
        models_completions: list[list[str]],
        criterion: str,
        prompt: str,
        model="gpt-4o-mini",
        batch_size=1000,
        n_iterations_per_model=50,
        ):
    
    # Evaluate completions
    battles = []
    for model_1 in range(len(models_completions)):
        for i in range(n_iterations_per_model):
            # sample opposing model
            model_2 = random.choice(range(len(models_completions)))
            # TODO: ensure model_1 != model_2
            completion_1 = random.choice(models_completions[model_1])
            completion_2 = random.choice(models_completions[model_2])
            battles.append((model_1, model_2, completion_1, completion_2))
            # battles.append((model_2, model_1, completion_2, completion_1))
    random.shuffle(battles)

    
    async def evaluate_all():
        tasks = [
            run_battle(text_1, text_2, criterion, prompt, client, model)
            for _, _, text_1, text_2 in battles
        ]
        # chunk tasks into batches
        chunked_tasks = [tasks[i:i+batch_size] for i in range(0, len(tasks), batch_size)]
        results = []
        for chunk in chunked_tasks:
            results.extend(await asyncio.gather(*chunk))
        return results
    
    results = asyncio.run(evaluate_all())

    filered_results = []
    for i, result in enumerate(results):
        result["model_1"] = battles[i][0]
        result["model_2"] = battles[i][1]
        if "error" not in result and "winner" in result:
            filered_results.append(result)
    
    return filered_results


def run_comparisons(
        text_pairs: list[tuple[str, str]],
        criterion: str,
        prompt: str,
        model="gpt-4o-mini",
        batch_size=1000,
        ):
    
    async def evaluate_all():
        tasks = [
            run_battle(text_1, text_2, criterion, prompt, client, model)
            for text_1, text_2 in text_pairs
        ]
        # chunk tasks into batches
        chunked_tasks = [tasks[i:i+batch_size] for i in range(0, len(tasks), batch_size)]
        results = []
        for chunk in chunked_tasks:
            results.extend(await asyncio.gather(*chunk))
        return results
    
    results = asyncio.run(evaluate_all())
    return results
    

# Example usage
if __name__ == "__main__":
    prompt = "Once upon a time,"
    evaluation_criterion = "humor and lightheartedness"
    completions = [
        "Robo tried dancing. It was clumsy but got better. Everyone laughed.",
        "Robo joined its owners dancing. It was stiff but made them laugh.",
    ]
    evaluations = evaluate_completions(completions, evaluation_criterion, prompt)
    for i, evaluation in enumerate(evaluations):
        print(f"Completion {i+1} Evaluation:\n{evaluation}\n")
