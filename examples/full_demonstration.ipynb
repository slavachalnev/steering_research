{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**A brief tour of some of the core methods around feature steering**"
      ],
      "metadata": {
        "id": "WoUmhMzp1k59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "amAFHJ6y4rWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/jbloomAus/SAELens\n",
        "%pip install transformer_lens\n",
        "%pip install datasets\n",
        "%pip install tqdm\n",
        "%pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYqKP_I54JuP",
        "outputId": "9b4a1ebb-c4f8-4d91-a53b-c5b98b2fa4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.0/920.0 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m339.9/339.9 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.1/296.1 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.1/221.1 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sae-lens (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py2store (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\n",
            "notebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 26.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: transformer_lens in /usr/local/lib/python3.10/dist-packages (1.19.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.31.0)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.20.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.30)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.0.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.7.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.4)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.41.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.12.2)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.17.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.15.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.9.5)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->transformer_lens) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (2.6.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T60M7zcc1f5C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.abspath('..'))\n",
        "\n",
        "from typing import Callable, Optional\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformer_lens import utils as tutils\n",
        "from transformer_lens.evals import make_pile_data_loader, evaluate_on_dataset\n",
        "\n",
        "from functools import partial\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sae_lens import SAE\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "## To access the Gemma model, need an access token (at least for collab)\n",
        "from huggingface_hub import login\n",
        "login(token=\"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\")\n",
        "\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = HookedTransformer.from_pretrained(\"gemma-2b\", device=device)"
      ],
      "metadata": {
        "id": "5JdNJSmG31kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def normalise_decoder(sae, scale_input=False):\n",
        "    \"\"\"\n",
        "    Normalises the decoder weights of the SAE to have unit norm.\n",
        "\n",
        "    Use this when loading for gemma-2b saes.\n",
        "\n",
        "    Args:\n",
        "        sae (SparseAutoencoder): The sparse autoencoder.\n",
        "        scale_input (bool): Use this when loading layer 12 model.\n",
        "    \"\"\"\n",
        "    norms = torch.norm(sae.W_dec, dim=1)\n",
        "    sae.W_dec /= norms[:, None]\n",
        "    sae.W_enc *= norms[None, :]\n",
        "    sae.b_enc *= norms"
      ],
      "metadata": {
        "id": "31z21QRY3355"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Somewhat soon, we want to be working with SAEs from multiple layers of Gemma 2b. Google Deepmind will be releasing a full set by approx July 14th, 2024."
      ],
      "metadata": {
        "id": "bUWANp_24wJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading in layer 6 SAE, there is also layer 11 sae and soon all layers\n",
        "hp6 = \"blocks.6.hook_resid_post\"\n",
        "\n",
        "sae6, _, _ = SAE.from_pretrained(\n",
        "    release = \"gemma-2b-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
        "    sae_id = hp6, # won't always be a hook point\n",
        "    device = 'cpu'\n",
        ")\n",
        "\n",
        "sae6 = sae6.to(device)\n",
        "normalise_decoder(sae6)"
      ],
      "metadata": {
        "id": "zLasgueM33c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting activations"
      ],
      "metadata": {
        "id": "goYOYUnNje5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def text_to_sae_feats(\n",
        "        model: HookedTransformer,\n",
        "        sae: SAE,\n",
        "        hook_point: str,\n",
        "        text: str,\n",
        "        return_logits = False,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Converts text to SAE features.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: SAE activations. Shape: [batch_size, sequence_len, d_sae]\n",
        "    \"\"\"\n",
        "    _, acts = model.run_with_cache(text, names_filter=hook_point, prepend_bos=True)\n",
        "\n",
        "    acts = acts[hook_point]\n",
        "\n",
        "    all_sae_acts = []\n",
        "    for batch in acts:\n",
        "        sae_acts = sae.encode(batch)\n",
        "        all_sae_acts.append(sae_acts)\n",
        "\n",
        "    return torch.stack(all_sae_acts, dim=0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def top_activations(activations: torch.Tensor, top_k: int=10):\n",
        "    \"\"\"\n",
        "    Get the top_k activations for each position in the sequence.\n",
        "\n",
        "    Returns:\n",
        "        top_v (torch.Tensor): Top k activations. Shape: [batch_size, sequence_len, top_k]\n",
        "        top_i (torch.Tensor): Top k indices. Shape: [batch_size, sequence_len, top_k]\n",
        "    \"\"\"\n",
        "    top_v, top_i = torch.topk(activations, top_k, dim=-1)\n",
        "\n",
        "    return top_v, top_i"
      ],
      "metadata": {
        "id": "AJrsQZa37CH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example usage\n",
        "\n",
        "`text_to_sae_feats` get raw feature activations for all token positions\n",
        "\n",
        "`top_activations` gets list of features sorted by score"
      ],
      "metadata": {
        "id": "dgqhaT1l9j7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sae_acts = text_to_sae_feats(model, sae6, hp6, \"The quick brown fox jumps over the lazy dog.\")\n",
        "print(sae_acts)\n",
        "\n",
        "top_v, top_i = top_activations(sae_acts)\n",
        "\n",
        "print(top_i)"
      ],
      "metadata": {
        "id": "ovZHPcRP8XgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def find_similar_features(target_idx, top_k=10, return_rows=True):\n",
        "    \"\"\"\n",
        "    Finds the top-k most similar rows (or columns) to a target row (or column) in a cosine similarity matrix.\n",
        "\n",
        "    Args:\n",
        "        cos_sim_matrix (torch.Tensor): The cosine similarity matrix.\n",
        "        target_idx (int): The index of the target row (or column) to compare against.\n",
        "        top_k (int, optional): The number of most similar rows (or columns) to return. Default is 10.\n",
        "        return_rows (bool, optional): If True, return the most similar rows. If False, return the most similar columns. Default is True.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor containing the indices of the top-k most similar rows (or columns).\n",
        "        torch.Tensor: A tensor containing the similarity scores of the top-k most similar rows (or columns).\n",
        "    \"\"\"\n",
        "    # Normalize the embeddings along the embedding dimension\n",
        "    normalized_embeddings = F.normalize(sae6.W_dec, p=2, dim=1)\n",
        "\n",
        "    # Calculate the cosine similarity matrix\n",
        "    cos_sim_matrix = torch.mm(normalized_embeddings, normalized_embeddings.t()).cuda()\n",
        "\n",
        "    target_vector = cos_sim_matrix[target_idx] if return_rows else cos_sim_matrix[:, target_idx]\n",
        "    similarities = target_vector if return_rows else target_vector.T\n",
        "\n",
        "    topk_similarities, topk_indices = torch.topk(similarities, k=top_k, largest=True, sorted=True)\n",
        "\n",
        "    if not return_rows:\n",
        "        topk_indices = topk_indices.T\n",
        "\n",
        "    return topk_indices, topk_similarities"
      ],
      "metadata": {
        "id": "DmbgzMgo7VKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topk_indices, topk_similarities = find_similar_features(10351) # 1062 is the feature for Anger\n",
        "\n",
        "print(topk_indices)"
      ],
      "metadata": {
        "id": "Vlkmjzt3_IfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When doing cosine similarity for feature `1058` (anger), you find feature `15989` which is an abusive language feature. (https://www.neuronpedia.org/gemma-2b/6-res-jb/15989)\n",
        "\n",
        "`10351` (intelligence) finds `5827` (smart), `4982` ([adept, skilled](https://www.neuronpedia.org/gemma-2b/6-res-jb/4982))\n",
        "\n",
        "Sometimes cosine similarity can return other similar feature but this method is hit or miss. For example, finding additional wedding features doesn't work as well."
      ],
      "metadata": {
        "id": "zNIOpqyaooi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intelligence = sae6.W_dec[10351]   # intelligence and genius\n",
        "writing = sae6.W_dec[1058]  # writing\n",
        "anger = sae6.W_dec[1062]  # anger\n",
        "london = sae6.W_dec[10138]  # London\n",
        "wedding = sae6.W_dec[8406]  # wedding\n",
        "broad_wedding = sae6.W_dec[2378] # broad wedding"
      ],
      "metadata": {
        "id": "iw91oqUv_eOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steering"
      ],
      "metadata": {
        "id": "ce-7H8Rxjhgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the simplest patching method\n",
        "# You insert the steering vector at all token positions, for all batches\n",
        "def patch_resid(resid, hook, steering, scale=1):\n",
        "    resid[:, :, :] = resid[:, :, :] + steering * scale\n",
        "    return resid\n",
        "\n",
        "\n",
        "def mix_vectors(feature_pairs):\n",
        "    # Initialize lists to store encoded and decoded feature vectors\n",
        "    steer_dec_list = []\n",
        "\n",
        "    # Iterate over each feature-strength pair\n",
        "    for feature, strength in feature_pairs:\n",
        "        # Calculate the steered encoded and decoded vectors\n",
        "        feature_steer = sae6.W_dec[feature, :] * strength\n",
        "\n",
        "        print(feature_steer.shape)\n",
        "        print(feature_steer)\n",
        "\n",
        "        # Append the vectors to the respective lists\n",
        "        steer_dec_list.append(feature_steer)\n",
        "\n",
        "    # Stack the lists to create the final tensors\n",
        "    steer_dec = torch.stack(steer_dec_list, dim=0)\n",
        "\n",
        "    return steer_dec.sum(dim=0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_basic(\n",
        "    model: HookedTransformer,\n",
        "    steer: tuple[str, Callable], # includes the steering hook.\n",
        "    prompt = \"\",\n",
        "    n_samples=4,\n",
        "    batch_size=4,\n",
        "    max_new_tokens=40,\n",
        "    top_k=50,\n",
        "    top_p=0.3,\n",
        "):\n",
        "    tokens = model.to_tokens(prompt, prepend_bos=True)\n",
        "    prompt_batch = tokens.expand(batch_size, -1)\n",
        "\n",
        "    results = []\n",
        "    num_batches = (n_samples + batch_size - 1) // batch_size  # Calculate number of batches\n",
        "\n",
        "    with model.hooks(fwd_hooks=[steer]):\n",
        "        for _ in tqdm(range(num_batches)):\n",
        "            batch_results = model.generate(\n",
        "                prompt_batch,\n",
        "                prepend_bos=True,\n",
        "                use_past_kv_cache=True,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                verbose=False,\n",
        "                top_k=top_k,\n",
        "                top_p=top_p,\n",
        "            )\n",
        "            batch_results = batch_results[:, 1:] # cut bos\n",
        "            str_results = model.to_string(batch_results)\n",
        "            results.extend(str_results)\n",
        "    return results[:n_samples]\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(\n",
        "    model: HookedTransformer,\n",
        "    hooks: list[tuple[str, Callable]], # includes the steering hook.\n",
        "    schedules: Optional[list[tuple[int, int]]] = None,\n",
        "    prompt = \"\",\n",
        "    n_samples=4,\n",
        "    batch_size=4,\n",
        "    max_new_tokens=40,\n",
        "    top_k=50,\n",
        "    top_p=0.3,\n",
        "):\n",
        "    if schedules is None:\n",
        "        schedules = [(None, None) for _ in hooks]\n",
        "\n",
        "    token_step = 0\n",
        "\n",
        "    def counter_hook(resid, hook):\n",
        "        # keeps track of which token we're up to\n",
        "        nonlocal token_step\n",
        "        token_step += 1\n",
        "        return resid\n",
        "\n",
        "    def updated_hook(resid, hook, hook_fn, start, end):\n",
        "        nonlocal token_step\n",
        "        if token_step >= start and token_step <= end:\n",
        "            return hook_fn(resid, hook)\n",
        "        return resid\n",
        "\n",
        "    new_hooks = []\n",
        "    for i, (hook_layer, hook_fn) in enumerate(hooks):\n",
        "        # we modify every hook_fn to only run when the token_step is within the schedule\n",
        "        start, end = schedules[i]\n",
        "        start = start if start is not None else 0\n",
        "        end = end if end is not None else max_new_tokens + 1\n",
        "        new_hooks.append((hook_layer, partial(updated_hook, start=start, end=end, hook_fn=hook_fn)))\n",
        "    new_hooks.append((\"blocks.0.hook_resid_post\", counter_hook))\n",
        "\n",
        "    tokens = model.to_tokens(prompt, prepend_bos=True)\n",
        "    prompt_batch = tokens.expand(batch_size, -1)\n",
        "\n",
        "    results = []\n",
        "    num_batches = (n_samples + batch_size - 1) // batch_size  # Calculate number of batches\n",
        "\n",
        "    with model.hooks(fwd_hooks=new_hooks):\n",
        "        for _ in range(num_batches):\n",
        "            batch_results = model.generate(\n",
        "                prompt_batch,\n",
        "                prepend_bos=True,\n",
        "                use_past_kv_cache=True,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                verbose=False,\n",
        "                top_k=top_k,\n",
        "                top_p=top_p,\n",
        "            )\n",
        "            batch_results = batch_results[:, 1:] # cut bos\n",
        "            str_results = model.to_string(batch_results)\n",
        "            results.extend(str_results)\n",
        "    return results[:n_samples]"
      ],
      "metadata": {
        "id": "K-raGpexgRLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following two examples demonstrate steering using the wedding feature we found in the first section scaled to 60. `generate_basic` and `generate` do the same thing in these two examples.\n",
        "\n",
        "However, we are working on improving the `generate` to allow for steering on multiple features over time. For example, if you first want to steer in one direction for X steps and then steer in another."
      ],
      "metadata": {
        "id": "6MtXXwy3n06u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_basic(\n",
        "    model,\n",
        "    (hp6, partial(patch_resid, steering=wedding, scale=60)),\n",
        "    \"I think\",\n",
        "    batch_size=64,\n",
        "    n_samples=5,\n",
        ")"
      ],
      "metadata": {
        "id": "DqFJPYfUiCGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\n",
        "    model,\n",
        "    hooks=[(hp6, partial(patch_resid, steering=wedding, scale=60))],\n",
        "    prompt=\"I think\",\n",
        "    batch_size=64,\n",
        "    n_samples=5,\n",
        ")"
      ],
      "metadata": {
        "id": "bTWyDV-irzkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-feature steering**\n",
        "\n",
        "Pass the features you want to steer with along with the scale factor for how strongly to steer in that direction\n",
        "\n",
        "Optimal scale factors for a variety of feature combinations were studied over a one week period and documented in this figma: https://www.figma.com/design/7csDOF7mDg1OiF6Na7dfin/Matt-%2B-Slava-Research-Report?node-id=0-1&t=rFDXzEIFcRQK3DBL-1\n",
        "\n",
        "There are some interesting implications of this research. Namely, that at any one time, it is unlikely that you can steer with more than two to three features at once and retain model coherence using our current method. This is because when the total added scale factor is greater than ~80, the model loses coherence. However, for a feature to have a chance to be expressed, it needs to be inserted at a minimum scale factor of 30-40. Hence, max 2-3 features inserted at once.\n",
        "\n",
        "It's worth noting here that even at the optimal scale factors, the output still has a high variance, sometimes not reflecting both features. This is something we are actively trying to improve – for both single and multi-feature steering."
      ],
      "metadata": {
        "id": "2FDmsDc0vQPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# single feature steering\n",
        "london = sae6.W_dec[10138]  # London\n",
        "wedding = sae6.W_dec[8406]  #  wedding\n",
        "\n",
        "# mixed feature steering\n",
        "london_wedding = mix_vectors([[10138, 40], [8406, 40]])"
      ],
      "metadata": {
        "id": "IeQNP1PsvS72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insert at scale factor 1 since the scale factor was added in during mixing\n",
        "generate(\n",
        "    model,\n",
        "    hooks=[(hp6, partial(patch_resid, steering=london_wedding, scale=1))],\n",
        "    prompt=\"I think\",\n",
        "    batch_size=64,\n",
        "    n_samples=10,\n",
        ")"
      ],
      "metadata": {
        "id": "9rkh7LeDybWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scheduling**\n",
        "\n",
        "Another goal for steering is the ability to steer on multiple directions in sequence.\n",
        "\n",
        "Our `generate` function allows for this kind of steering by specifying for which range of tokens a particular steering vector should have influence.\n",
        "\n",
        "To do this, you can pass in multiple hooks along with a list of schedules.\n",
        "\n",
        "Schedules indicate how long each hook should be steering based on token count. A `None` parameter on the beginning or end is like extending the influence of that steering hook to the very start or end.\n",
        "\n",
        "Due to not resetting the `kv_cache` in the `generate` function (which is how production systems would operate), early steering vectors continue to influence later text, making it harder for later steering directions to have an effect. We are working on improving this while keeping generation efficient."
      ],
      "metadata": {
        "id": "IRlGCu601GYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hooks = [\n",
        "    (hp6, partial(patch_resid, steering=london, scale=70)),\n",
        "    (hp6, partial(patch_resid, steering=wedding, scale=70)),\n",
        "]\n",
        "\n",
        "generate(model,\n",
        "        hooks=hooks,\n",
        "        schedules=[(1, 15), (16, None)],\n",
        "        max_new_tokens=50,\n",
        "        prompt=\"I think\",\n",
        "         )"
      ],
      "metadata": {
        "id": "bAjExfO11CRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evals\n",
        "\n",
        "We have developed, and are developing, a number of eval methods which aim to complement each other.\n",
        "\n",
        "TODO: document here"
      ],
      "metadata": {
        "id": "jXBRU2np6jNK"
      }
    }
  ]
}