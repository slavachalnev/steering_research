{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f4491cb6c20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tutils\n",
    "from transformer_lens.evals import make_pile_data_loader, evaluate_on_dataset\n",
    "\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sae_lens import SAE\n",
    "# from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "# from sae_lens import SparseAutoencoder, ActivationsStore\n",
    "\n",
    "from steering.evals_utils import run_comparisons\n",
    "from steering.utils import normalise_decoder\n",
    "from steering.patch import generate, scores_2d, patch_resid\n",
    "\n",
    "# from sae_vis.data_config_classes import SaeVisConfig\n",
    "# from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9bd75e0f524dca9e0b4736b6566f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp6 = \"blocks.6.hook_resid_post\"\n",
    "\n",
    "sae6, _, _ = SAE.from_pretrained(\n",
    "    release = \"gemma-2b-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = hp6, # won't always be a hook point\n",
    "    device = 'cpu'\n",
    ")\n",
    "\n",
    "sae6 = sae6.to(device)\n",
    "normalise_decoder(sae6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we want?\n",
    "\n",
    "# 50 steering vectors\n",
    "\n",
    "# 2048 generations per steering vector\n",
    "# this is 1024 pairs.\n",
    "\n",
    "# filter pairs such that one of the texts is preffered in both coherence and score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vectors = [\n",
    "    {\"ft_id\": 1062, \"ft_desc\": \"Anger\", \"eval_criterion\": \"Text is angry or mentions anger/frustration or anything related to anger\"},\n",
    "    {\"ft_id\": 10138, \"ft_desc\": \"London\", \"eval_criterion\": \"Mentions London or anything related to London\"},\n",
    "    {\"ft_id\": 8406, \"ft_desc\": \"Wedding\", \"eval_criterion\": \"Mentions weddings or anything related to weddings\"},\n",
    "    {\"ft_id\": 2378, \"ft_desc\": \"Broad Wedding\", \"eval_criterion\": \"Mentions weddings or anything related to weddings\"},\n",
    "    {\"ft_id\": 1058, \"ft_desc\": \"Writing\", \"eval_criterion\": \"Mentions writing or anything related to writing\"},\n",
    "    {\"ft_id\": 10298, \"ft_desc\": \"Death\", \"eval_criterion\": \"Mentions death\"},\n",
    "    {\"ft_id\": 11309, \"ft_desc\": \"Rome\", \"eval_criterion\": \"Mentions Rome or anything related to Rome\"},\n",
    "    {\"ft_id\": 2324, \"ft_desc\": \"Dragons\", \"eval_criterion\": \"Mentions dragons or anything related to dragons\"},\n",
    "    {\"ft_id\": 10473, \"ft_desc\": \"Knight\", \"eval_criterion\": \"Mentions knights or medieval themes\"},\n",
    "    {\"ft_id\": 15249, \"ft_desc\": \"Hurt\", \"eval_criterion\": \"Mentions hurt in physical or emotional sense\"},\n",
    "    {\"ft_id\": 4458, \"ft_desc\": \"Pain\", \"eval_criterion\": \"Mentions physical pain\"},\n",
    "    {\"ft_id\": 13056, \"ft_desc\": \"Christian\", \"eval_criterion\": \"Text mentions christianity or is related to christianity\"},\n",
    "    {\"ft_id\": 7095, \"ft_desc\": \"Suicide\", \"eval_criterion\": \"Text mentions suicide or is suicidal\"},\n",
    "    {\"ft_id\": 4303, \"ft_desc\": \"Conspiracy\", \"eval_criterion\": \"Mentions conspiracy\"},\n",
    "\n",
    "    {\"ft_id\": 6, \"ft_desc\": \"New Orleans\", \"eval_criterion\": \"Mentions New Orleans or anything related to New Orleans\"},\n",
    "    {\"ft_id\": 7, \"ft_desc\": \"Lost\", \"eval_criterion\": \"Mentions losing things\"},\n",
    "    {\"ft_id\": 12, \"ft_desc\": \"Harry Potter\", \"eval_criterion\": \"Mentions Harry Potter or anything related to Harry Potter\"},\n",
    "    {\"ft_id\": 14, \"ft_desc\": \"Toys\", \"eval_criterion\": \"Mentions toys\"},\n",
    "    {\"ft_id\": 21, \"ft_desc\": \"Back then\", \"eval_criterion\": \"Mentions the past, e.g. back in the early days.\"},\n",
    "    {\"ft_id\": 31, \"ft_desc\": \"Office devices\", \"eval_criterion\": \"Mentions printer or router or generator or transducer etc.\"},\n",
    "    {\"ft_id\": 32, \"ft_desc\": \"Family Event\", \"eval_criterion\": \"Descriptions of family-friendly events.\"},\n",
    "]\n",
    "\n",
    "coherence_criterion = \"Text is coherent, the grammar is correct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [50, 55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each steering vector,\n",
    "#   for each scale generate 512 texts, merge into big list (of tensors?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def gen_and_rate(vec_index, scales, prompt=\"I think\"):\n",
    "\n",
    "    score_crit = steering_vectors[vec_index]['eval_criterion']\n",
    "\n",
    "    steer = sae6.W_dec[steering_vectors[vec_index]['ft_id']]\n",
    "    n_batches_per_scale = 16\n",
    "    batch_size = 64\n",
    "\n",
    "    prompt_tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "    prompt_batch = prompt_tokens.expand(batch_size, -1)\n",
    "\n",
    "    gen_tokens = []\n",
    "    for scale in scales:\n",
    "        with model.hooks([(hp6, partial(patch_resid, steering=steer, scale=scale))]):\n",
    "            for _ in range(n_batches_per_scale):\n",
    "                batch_results = model.generate(\n",
    "                    prompt_batch,\n",
    "                    prepend_bos=True,\n",
    "                    use_past_kv_cache=True,\n",
    "                    max_new_tokens=29, # 32 - 3\n",
    "                    verbose=False,\n",
    "                    top_k=50,\n",
    "                    top_p=0.3,\n",
    "                )\n",
    "                gen_tokens.append(batch_results)\n",
    "    \n",
    "    gen_tokens = torch.cat(gen_tokens, dim=0)\n",
    "\n",
    "    # Shuffle gen_tokens along dimension 0\n",
    "    num_samples = gen_tokens.shape[0]\n",
    "    shuffled_indices = torch.randperm(num_samples)\n",
    "    gen_tokens = gen_tokens[shuffled_indices]\n",
    "\n",
    "    all_texts = model.to_string(gen_tokens)\n",
    "\n",
    "    # first half paired up with second half\n",
    "    texts_a = all_texts[:num_samples//2]\n",
    "    texts_b = all_texts[num_samples//2:]\n",
    "    c_ratings = run_comparisons(text_pairs=list(zip(texts_a, texts_b)), criterion=coherence_criterion, prompt=prompt)\n",
    "    s_ratings = run_comparisons(text_pairs=list(zip(texts_a, texts_b)), criterion=score_crit, prompt=prompt)\n",
    "\n",
    "    def convert_to_binary(rating):\n",
    "        if isinstance(rating, dict):\n",
    "            if 'error' in rating or rating.get('winner') == 'tie':\n",
    "                return None\n",
    "            return 1 if rating['winner'] == 'A' else 0\n",
    "        return None\n",
    "\n",
    "    c_binary = [convert_to_binary(r) for r in c_ratings]\n",
    "    s_binary = [convert_to_binary(r) for r in s_ratings]\n",
    "    \n",
    "    valid_indices = [i for i, (c, s) in enumerate(zip(c_binary, s_binary)) \n",
    "                     if c is not None and s is not None and c == s]\n",
    "    \n",
    "    texts_a = [texts_a[i] for i in valid_indices]\n",
    "    texts_b = [texts_b[i] for i in valid_indices]\n",
    "    valid_tokens = gen_tokens[valid_indices + [i + num_samples//2 for i in valid_indices]]\n",
    "    \n",
    "    c_ratings = [c_binary[i] for i in valid_indices]\n",
    "    s_ratings = [s_binary[i] for i in valid_indices]\n",
    "\n",
    "    return valid_tokens, texts_a, texts_b, c_ratings\n",
    "\n",
    "\n",
    "# valid_tokens, _, _, ratings = gen_and_rate(0, scales=[0, 40, 60]) ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [37:30<00:00, 107.18s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_data(scales, prompt=\"I think\"):\n",
    "    all_wins = []\n",
    "    all_losses = []\n",
    "    vector_idxs = []\n",
    "\n",
    "    for i in tqdm(range(len(steering_vectors))):\n",
    "        valid_tokens, _, _, ratings = gen_and_rate(i, scales=scales, prompt=prompt)\n",
    "        vector_idxs.extend([i] * len(ratings))\n",
    "\n",
    "        tokens_win = []\n",
    "        tokens_loss = []\n",
    "\n",
    "        for i, r in enumerate(ratings):\n",
    "            if r == 1:\n",
    "                tokens_win.append(valid_tokens[i])\n",
    "                tokens_loss.append(valid_tokens[i + len(ratings)])\n",
    "            else:\n",
    "                tokens_win.append(valid_tokens[i + len(ratings)])\n",
    "                tokens_loss.append(valid_tokens[i])\n",
    "\n",
    "        tokens_win = torch.stack(tokens_win)\n",
    "        tokens_loss = torch.stack(tokens_loss)\n",
    "\n",
    "        all_wins.append(tokens_win)\n",
    "        all_losses.append(tokens_loss)\n",
    "\n",
    "\n",
    "    all_wins = torch.cat(all_wins, dim=0)\n",
    "    all_losses = torch.cat(all_losses, dim=0)\n",
    "\n",
    "    # shuffle\n",
    "    num_samples = all_wins.shape[0]\n",
    "    shuffled_indices = torch.randperm(num_samples)\n",
    "    all_wins = all_wins[shuffled_indices]\n",
    "    all_losses = all_losses[shuffled_indices]\n",
    "    vector_idxs = [vector_idxs[i] for i in shuffled_indices]\n",
    "\n",
    "    return all_wins, all_losses, vector_idxs\n",
    "\n",
    "    \n",
    "    \n",
    "wins, losses, vector_idxs = get_data(scales=scales)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"comparison_data\"\n",
    "\n",
    "os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "torch.save(wins, os.path.join(dir, \"wins.pt\"))\n",
    "torch.save(losses, os.path.join(dir, \"losses.pt\"))\n",
    "\n",
    "with open(os.path.join(dir, \"vector_idxs.json\"), \"w\") as f:\n",
    "    json.dump(vector_idxs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save steering vectors json\n",
    "with open(os.path.join(dir, \"steering_vectors.json\"), \"w\") as f:\n",
    "    json.dump(steering_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
