{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb8c4139600>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tutils\n",
    "from transformer_lens.evals import make_pile_data_loader, evaluate_on_dataset\n",
    "\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sae_lens import SAE\n",
    "# from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "# from sae_lens import SparseAutoencoder, ActivationsStore\n",
    "\n",
    "from steering.evals_utils import run_comparisons\n",
    "from steering.utils import normalise_decoder\n",
    "from steering.patch import generate, scores_2d, patch_resid\n",
    "\n",
    "# from sae_vis.data_config_classes import SaeVisConfig\n",
    "# from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55a5cc86dbb42d7bfca48f16c79e276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp6 = \"blocks.6.hook_resid_post\"\n",
    "\n",
    "sae6, _, _ = SAE.from_pretrained(\n",
    "    release = \"gemma-2b-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = hp6, # won't always be a hook point\n",
    "    device = 'cpu'\n",
    ")\n",
    "\n",
    "sae6 = sae6.to(device)\n",
    "normalise_decoder(sae6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we want?\n",
    "\n",
    "# 50 steering vectors\n",
    "\n",
    "# 2048 generations per steering vector\n",
    "# this is 1024 pairs.\n",
    "\n",
    "# filter pairs such that one of the texts is preffered in both coherence and score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vectors = [\n",
    "    {\"ft_id\": 1062, \"ft_desc\": \"Anger\", \"eval_criterion\": \"Text is angry or mentions anger/frustration or anything related to anger\"},\n",
    "    # {\"ft_id\": 10138, \"ft_desc\": \"London\", \"eval_criterion\": \"Mentions London or anything related to London\"},\n",
    "    # {\"ft_id\": 8406, \"ft_desc\": \"Wedding\", \"eval_criterion\": \"Mentions weddings or anything related to weddings\"},\n",
    "    # {\"ft_id\": 2378, \"ft_desc\": \"Broad Wedding\", \"eval_criterion\": \"Mentions weddings or anything related to weddings\"},\n",
    "    {\"ft_id\": 1058, \"ft_desc\": \"Writing\", \"eval_criterion\": \"Mentions writing or anything related to writing\"},\n",
    "    {\"ft_id\": 10298, \"ft_desc\": \"Death\", \"eval_criterion\": \"Mentions death\"},\n",
    "    {\"ft_id\": 11309, \"ft_desc\": \"Rome\", \"eval_criterion\": \"Mentions Rome or anything related to Rome\"},\n",
    "    {\"ft_id\": 2324, \"ft_desc\": \"Dragons\", \"eval_criterion\": \"Mentions dragons or anything related to dragons\"},\n",
    "    {\"ft_id\": 10473, \"ft_desc\": \"Knight\", \"eval_criterion\": \"Mentions knights or medieval themes\"},\n",
    "    {\"ft_id\": 15249, \"ft_desc\": \"Hurt\", \"eval_criterion\": \"Mentions hurt in physical or emotional sense\"},\n",
    "    {\"ft_id\": 4458, \"ft_desc\": \"Pain\", \"eval_criterion\": \"Mentions physical pain\"},\n",
    "    {\"ft_id\": 13056, \"ft_desc\": \"Christian\", \"eval_criterion\": \"Text mentions christianity or is related to christianity\"},\n",
    "    {\"ft_id\": 7095, \"ft_desc\": \"Suicide\", \"eval_criterion\": \"Text mentions suicide or is suicidal\"},\n",
    "    {\"ft_id\": 4303, \"ft_desc\": \"Conspiracy\", \"eval_criterion\": \"Mentions conspiracy\"},\n",
    "]\n",
    "\n",
    "coherence_criterion = \"Text is coherent, the grammar is correct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [0, 20, 40, 60, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each steering vector,\n",
    "#   for each scale generate 512 texts, merge into big list (of tensors?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating. Now rating\n",
      "[{'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}]\n",
      "[{'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'tie'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'A'}, {'winner': 'B'}, {'winner': 'B'}, {'winner': 'B'}]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def gen_and_rate(vec_index, scales, prompt=\"I think\"):\n",
    "\n",
    "    score_crit = steering_vectors[vec_index]['eval_criterion']\n",
    "\n",
    "    steer = sae6.W_dec[steering_vectors[vec_index]['ft_id']]\n",
    "    n_batches_per_scale = 2 ###\n",
    "    batch_size = 64\n",
    "\n",
    "    prompt_tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "    prompt_batch = prompt_tokens.expand(batch_size, -1)\n",
    "\n",
    "    gen_tokens = []\n",
    "    for scale in scales:\n",
    "        with model.hooks([(hp6, partial(patch_resid, steering=steer, scale=scale))]):\n",
    "            for _ in range(n_batches_per_scale):\n",
    "                batch_results = model.generate(\n",
    "                    prompt_batch,\n",
    "                    prepend_bos=True,\n",
    "                    use_past_kv_cache=True,\n",
    "                    max_new_tokens=29, # 32 - 3\n",
    "                    verbose=False,\n",
    "                    top_k=50,\n",
    "                    top_p=0.3,\n",
    "                )\n",
    "                gen_tokens.append(batch_results)\n",
    "    \n",
    "    gen_tokens = torch.cat(gen_tokens, dim=0)\n",
    "\n",
    "    # Shuffle gen_tokens along dimension 0\n",
    "    num_samples = gen_tokens.shape[0]\n",
    "    shuffled_indices = torch.randperm(num_samples)\n",
    "    gen_tokens = gen_tokens[shuffled_indices]\n",
    "\n",
    "    all_texts = model.to_string(gen_tokens)\n",
    "\n",
    "    print('done generating. Now rating')\n",
    "\n",
    "    # first half paired up with second half\n",
    "    texts_a = all_texts[:num_samples//2]\n",
    "    texts_b = all_texts[num_samples//2:]\n",
    "    c_ratings = run_comparisons(text_pairs=list(zip(texts_a, texts_b)), criterion=coherence_criterion, prompt=prompt)\n",
    "    s_ratings = run_comparisons(text_pairs=list(zip(texts_a, texts_b)), criterion=score_crit, prompt=prompt)\n",
    "\n",
    "    print(c_ratings)\n",
    "    print(s_ratings)\n",
    "\n",
    "gen_and_rate(0, scales=[0, 40, 60]) ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
